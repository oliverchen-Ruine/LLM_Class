## 1.环境依赖及说明
本节中的开发环境中所需如下 Python 库：

| 库名称       | 用途说明                         |
| --------- | ---------------------------- |
| **faiss** | 向量数据库核心库，用于构建索引与执行相似性搜索      |
| **numpy** | 数值计算库，用于处理向量数据，如生成/转换/归一化等操作 |
| **pickle** | Python 标准库，用于对对象进行序列化和反序列化，以便保存和读取文本映射数据 |

## 2.FAISS索引与文本映射数据的持久化

### 2.1 索引持久化

```python
from ollama import Client
import numpy as np
import faiss

client = Client(host='http://localhost:11434')
 # 调用嵌入模型生成词向量
def get_embedding(text, model="dengcao/Qwen3-Embedding-8B:Q5_K_M"):
    response = client.embeddings(model=model, prompt=text)
    return np.array(response['embedding'], dtype=np.float32)

# 创建示例向量数据
texts = ["神经网络原理", "深度学习框架", "大模型训练"]
vectors = np.array([get_embedding(t) for t in texts], dtype='float32')

# 创建一个FAISS索引（这里使用最简单的IndexFlatL2索引）
dimension = vectors[0].shape[0]
index = faiss.IndexFlatL2(dimension)

# 添加向量到索引
index.add(vectors)

# 将索引保存到文件
faiss.write_index(index, "my_faiss_index.faiss")

print("FAISS索引已保存到 my_faiss_index.faiss")
```

> 说明：文件`my_faiss_index.faiss`保存了向量数据和索引结构，文件大小依赖于向量数量和维度。

### 2.2 文本映射数据持久化 
>向量数据库只保存向量，不包含对应的文本信息。为了在检索时能够找到对应的文本，需要另外保存文本列表。
推荐使用Python的`pickle`库进行序列化保存。
```python
import pickle

texts = ["神经网络原理", "深度学习框架", "大模型训练"]

# 将文本列表保存到文件
with open("text_mapping.pkl", "wb") as f:
    pickle.dump(texts, f)

print("文本映射已保存到 text_mapping.pkl")
```
---

## 3.向量数据库检索
### 3.1 加载索引和文本映射数据
加载之前保存的索引文件，快速恢复索引状态：

```python
import faiss

loaded_index = faiss.read_index("my_faiss_index.faiss")

print(f"索引加载成功，当前向量数量：{loaded_index.ntotal}")
```
加载保存的文本列表：

```python
import pickle

with open("text_mapping.pkl", "rb") as f:
    loaded_texts = pickle.load(f)

print(f"加载的文本列表：{loaded_texts}")
```
### 3.2 检索向量
当我们有查询文本时，可以先将查询文本转成向量（用Embedding模型），再用加载的FAISS索引搜索最相似的向量，最后通过索引映射找到对应的文本。
```python
import numpy as np


query = "大语言模型"
# query_vec是查询文本的向量（float32 numpy数组，维度与索引一致）
query_vec = get_embedding(query)

k = 1  # 取最相似的1个结果
D, I = loaded_index.search(np.array([query_vec]), k)

print("距离：", D)
print("索引位置：", I)

for idx in I[0]:
    print(f"匹配文本：{loaded_texts[idx]}")
```
```
> 说明：`D`是距离，`I`是索引位置，`I[0]`表示取第一个查询结果。
```

### 3.3 元数据规范
```json
[
  {
    "id": "kb_001",
    "text": "神经网络基础原理",
    "source": "AI教材.pdf",
    "page": 45,
    "updated": "2025-03-15"
  },
  {
    "id": "kb_002",
    "text": "PyTorch实战指南",
    "source": "深度学习手册.docx",
    "chapter": 3
  },
  ...
]
```

### 3.4 知识库更新策略
1. **增量更新**：
   ```python
   new_vectors = np.array([get_embedding(new_text)])
   index.add(new_vectors)
   ```
 
2. **全量重建**：
   ```python
   # 当数据变动超过30%时触发
   if change_ratio > 0.3:
       rebuild_full_index()
   ```

### 3.5. 小结

* **索引文件（.faiss）** 存储向量数据和索引结构，便于快速加载。
* **文本映射文件（.pkl）** 存储与向量对应的原始文本，支持检索结果的解码。
* 通过同时保存和加载两部分数据，保证检索系统的完整性与稳定性。
* 在实际生产环境，可进一步对文件进行版本管理、加密或放入数据库中，提高安全性和扩展性。

---

## 4.总结
### 4.1 知识库系统架构
<p align="center"><img src="/img/2.4.1.png" height="600"></p>

>在拿到原始文档后，我们首先进行文本**清洗和分块**，然后使用Embedding模型将文本转换为**向量**，最后使用FAISS索引存储向量。当用户查询时，我们使用Embedding模型将查询**文本转换为向量**，然后使用FAISS索引找到**最相似的向量**，最后通过**索引映射**找到对应的文本。

### 4.2 核心模块与依赖

```python
import re
from typing import List, Dict, Any
import fitz  # PyMuPDF
import os
import json
import faiss # 向量数据库
import numpy as np
from dotenv import load_dotenv
from ollama import Client
import markdown2  # 用于解析MD文件
from bs4 import BeautifulSoup  # 用于从HTML中提取文本
```
- **fitz (PyMuPDF)**：用于PDF文件处理
- **markdown2/BeautifulSoup**：用于解析Markdown文件
- **ollama**：调用本地大模型
- **faiss**：构建和管理向量索引

### 4.3 核心函数
**文本分块函数:**（简易版）
```python
def split_text_into_chunks(text: str, max_tokens: int = 500, overlap: int = 50) -> List[str]:
    cleaned = re.sub(r'\s+', ' ', text)  # 修正正则表达式，将s+改为\s+
    sentences = re.split(r'(?<=[。！？.!?])', cleaned)

    chunks = []
    chunk = ''
    for sentence in sentences:
        if len(chunk) + len(sentence) <= max_tokens:
            chunk += sentence
        else:
            chunks.append(chunk)
            chunk = chunk[-overlap:] + sentence

    if chunk:
        chunks.append(chunk)
    return chunks
```
- **功能**：将文本按句子分割并组合成指定长度的块
- **参数**：
  - `max_tokens`：每个块的最大长度（字符数）
  - `overlap`：块之间的重叠长度，确保上下文连贯性

**向量生成与索引构建：**
```python
def generate_embeddings(texts: List[str], model: str = "dengcao/Qwen3-Embedding-8B:Q5_K_M", batch_size: int = 10) -> List[List[float]]:
    all_embeddings = []
    
    # 分批处理文本
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        # 调用嵌入模型生成词向量
        response = client.embeddings.create(
            prompt=batch,
            model=model
        )
        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)
    return all_embeddings

def create_faiss_index(embeddings: List[List[float]], index_path: str) -> None:
    import faiss
    vectors = np.array(embeddings, dtype='float32')
    dimension = len(embeddings[0])
    index = faiss.IndexFlatL2(dimension)  # 使用L2距离（欧氏距离）
    index.add(vectors)
    faiss.write_index(index, index_path)
```
- **流程**：
  1. 将文本分批调用API生成向量
  2. 使用FAISS创建索引（本例使用精确搜索的`IndexFlatL2`）
  3. 将索引保存到文件

  **主流程：**

```python
def process_single_file(file_path: str, output_dir: str) -> None:
    # 1. 提取文本（如MD格式）
    if file_ext == '.md':
        text = extract_text_from_md(file_path)
    
    # 2. 分割文本
    chunks = split_text_into_chunks(text, max_tokens=400, overlap=50)
    
    # 3. 生成向量
    embeddings = generate_embeddings(chunks)
    
    # 4. 保存文本块和向量
    save_chunks_to_json(chunks, output_json_path)
    save_embeddings_to_json(chunks_with_metadata, embeddings, vector_json_path)
    
    # 5. 创建FAISS索引
    create_faiss_index(embeddings, faiss_index_path)
```

### 4.4 可调整参数详解

### 文本处理参数
| 参数 | 所在函数 | 默认值 | 调整意义 |
|------|----------|--------|----------|
| `max_tokens` | `split_text_into_chunks` | 500 | 控制每个文本块的最大长度，影响检索精度和上下文连贯性 |
| `overlap` | `split_text_into_chunks` | 50 | 相邻块之间的重叠长度，确保语义连续性 |


### 向量生成参数
| 参数 | 所在函数 | 默认值 | 调整意义 |
|------|----------|--------|----------|
| `model` | `generate_embeddings` | "dengcao/Qwen3-Embedding-8B:Q5_K_M" | 指定Embedding模型，不同模型性能和成本不同 |
| `batch_size` | `generate_embeddings` | 10 | 每次API调用处理的文本数量，影响处理速度和资源消耗 |


### FAISS索引参数
| 参数 | 所在函数 | 默认值 | 调整意义 |
|------|----------|--------|----------|
| 索引类型 | `create_faiss_index` | IndexFlatL2 | 决定索引的检索速度和精度，如改用IndexHNSWFlat可提高速度 |


## 附录：知识库构建清单

### 实施步骤
1. [ ] 文档收集与清洗
2. [ ] 文本分块策略设计
3. [ ] Embedding模型选择
4. [ ] FAISS索引配置
5. [ ] 元数据系统设计
6. [ ] 持久化存储方案
7. [ ] 更新维护机制

### 常见问题排查
| 问题 | 可能原因 | 解决方案 |
|------|----------|----------|
| 高查询延迟 | 索引未优化 | 使用IVF/HNSW索引 |
| 低结果相关性 | Embedding模型不匹配 | 更换专用模型 |
| 内存溢出 | 向量维度过高 | 降维或分布式存储 |

> **关键建议**：每周运行`index.reconstruct_n(0, 1000)`采样检查索引完整性