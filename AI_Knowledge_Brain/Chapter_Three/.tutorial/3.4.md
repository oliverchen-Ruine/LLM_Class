通过上一节的学习，我们已经搭建了本地AI知识脑，但在实际部署和使用中，会面临性能瓶颈、响应延迟、资源限制等挑战，距离企业级应用还有一段距离。

**主要调优方向：**

* **优化检索阶段性能**

  检索是RAG的瓶颈点（占时60%以上），重点提升召回效率与精准度：

  * 传统精确检索（如暴力搜索）复杂度为O(N)，HNSW可降至O(log N)，内存占用降低50%+
  * 选用量化索引（如PQ/SCANN） 替代精确索引，或结合分层可导航小世界图（HNSW） 加速相似搜索。
  * 利用使用FAISS的IndexHNSWFlat或IVFPQ索引，提升检索速度和扩展性。
  * 利用GPU的并行计算能力，提升检索效率。

* **调用大模型接口的优化**

  * 合理控制上下文长度，避免Token超限
  * 缓存常见问题的回答，降低接口请求量
  * 异步请求与超时重试机制，提升系统鲁棒性

* **多线程与异步设计**

  * 并行处理用户请求，提高吞吐量
  * 异步IO避免接口阻塞，提高响应速度

* **日志与监控**

  * 关键流程日志记录，方便问题追踪
  * 性能指标监控，发现瓶颈及时调整

---

### 代码示例

以下示例演示如何用FAISS的IndexHNSWFlat索引替代简单的IndexFlatL2以提升大规模检索效率，同时示范简单的批量embedding调用示例：

```python
import faiss
import numpy as np
from typing import List

def create_hnsw_index(dim: int, M: int = 32, efConstruction: int = 128, efSearch: int = 64) -> faiss.Index:
    """
    创建并配置HNSW索引
    
    参数:
        dim (int): 向量维度
        M (int): HNSW节点连接数（影响图结构），默认32
        efConstruction (int): 构建时的搜索深度，默认128
        efSearch (int): 查询时的搜索深度，默认64
        
    返回:
        faiss.Index: 配置好的HNSW索引
    """
    # 创建HNSW Flat索引（使用L2距离）
    index = faiss.IndexHNSWFlat(dim, M)
    
    # 设置构建参数
    index.hnsw.efConstruction = efConstruction
    
    # 设置查询参数
    index.hnsw.efSearch = efSearch
    
    # 启用构建日志
    index.verbose = True
    
    print(f"HNSW索引创建完成: M={M}, efConstruction={efConstruction}, efSearch={efSearch}")
    
    return index

client = Client(host='http://localhost:11434')
 
def get_batch_embeddings(texts: List[str], model="dengcao/Qwen3-Embedding-8B:Q5_K_M", batch_size=32) -> List[np.ndarray]:
    """
    批量获取文本的embedding
    
    参数:
        texts: 文本列表
        model: 使用的embedding模型
        batch_size: 每批处理的文本数量
        
    返回:
        embedding数组列表
    """
    embeddings = []
    
    # 分批处理
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        try:
            # 批量请求embedding
            response = client.embeddings(
                model=model,
                prompt=batch  # 直接传入批量的文本
            )
            
            # 处理返回结果
            batch_embeddings = [np.array(embedding, dtype=np.float32) for embedding in response['embeddings']]
            embeddings.extend(batch_embeddings)
        except Exception as e:
            print(f"Error processing batch {i//batch_size}: {str(e)}")
            # 如果批量失败，回退到逐条处理
            for text in batch:
                try:
                    embedding = get_single_embedding(text, model)
                    embeddings.append(embedding)
                except Exception as e:
                    print(f"Error processing text '{text[:50]}...': {str(e)}")
                    embeddings.append(np.zeros(0))  # 返回空数组作为占位
    
    return embeddings
```

---

### 这么做的优势

* **IndexHNSWFlat索引提升检索速度和扩展性**

  * 对于高维数据，低延迟查询，ndexHNSWFlat索引时基于图的近似搜索，高召回率，无需训练
* **批量embedding减少调用次数**

  * 单条调用带来接口延迟，批量请求能充分利用接口并发能力，提升整体效率。
* **多线程/异步支持系统高并发**

  * 保证多用户同时访问时，系统依然响应及时，避免请求阻塞。
* **监控和日志保障系统稳定性**

  * 实时发现并修复异常，提高系统可用率。

---

### 可调节点

* **HNSW 参数配置**

  | 参数 | 默认值 | 作用 | 影响规律 |
  |------|--------|------|----------|
  | **`M`** | 16-64 | 每个节点的最大连接数 | ↗ 值增大 → 图更密集，召回率↑，内存占用↑，构建时间↑ |
  | **`efConstruction`** | 40-200 | 构建时的近邻搜索深度 | ↗ 值增大 → 索引质量↑，构建时间↑，内存临时消耗↑ |
  | **`efSearch`**** | 16-128 | 查询时的搜索深度 | ↗ 值增大 → 召回率↑，查询延迟↑ |


* **批量大小调整**

  * batch\_size可根据调用速率限制情况调整，保证请求效率与稳定性。
* **索引更新策略**

  * 定期重训练索引或增量添加，避免索引陈旧影响检索效果。
* **缓存策略**

  * 缓存热点查询或生成结果，避免重复计算。
* **日志级别和监控指标配置**

  * 根据部署环境灵活调整，保证不影响性能同时能有效定位问题。

---

>通过本教学案例学习，可构建端到端的本地RAG系统，实现专业文档的高效知识管理与智能问答，同时平衡性能与准确性需求。