{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 该实例针对markdown文件处理格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import markdown2 \n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"自动检测文本主要语言（中文/英文）\"\"\"\n",
    "    chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', text))\n",
    "    english_chars = len(re.findall(r'[a-zA-Z]', text))\n",
    "    return 'zh' if chinese_chars > english_chars else 'en'\n",
    "\n",
    "def split_text_into_chunks(\n",
    "        text: str,\n",
    "        max_tokens: int = 500,\n",
    "        overlap: int = 50,\n",
    "        language: str = None\n",
    ") -> List[str]:\n",
    "    \"\"\"将长文本切分为多个chunk，保留单词完整性并支持重叠\n",
    "\n",
    "    Args:\n",
    "        text: 输入文本\n",
    "        max_tokens: 每个chunk的最大长度\n",
    "        overlap: 相邻chunk之间的重叠长度\n",
    "        language: 可选强制指定语言('zh'/'en')，默认自动检测\n",
    "\n",
    "    Returns:\n",
    "        切分后的chunk列表\n",
    "    \"\"\"\n",
    "    # 清理文本并检测语言\n",
    "    cleaned = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if not cleaned:\n",
    "        return []\n",
    "\n",
    "    lang = language or detect_language(cleaned)\n",
    "\n",
    "    # 中英文不同的分割逻辑\n",
    "    if lang == 'zh':\n",
    "        # 中文按句子分割（保留标点）\n",
    "        sentences = [s for s in re.split(r'(?<=[。！？.!?])', cleaned) if s]\n",
    "    else:\n",
    "        # 英文按句子分割（保留单词完整性）\n",
    "        sentences = [s for s in re.split(r'(?<=[.!?])\\s+', cleaned) if s]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # 处理超长句子（超过max_tokens）\n",
    "        if len(sentence) > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "\n",
    "            if lang == 'zh':\n",
    "                # 中文按字符分割（保留重叠）\n",
    "                chunks.extend([sentence[i:i + max_tokens]\n",
    "                               for i in range(0, len(sentence), max_tokens - overlap)])\n",
    "            else:\n",
    "                # 英文按单词分割（保留单词完整性）\n",
    "                words = sentence.split()\n",
    "                current_words = []\n",
    "                for word in words:\n",
    "                    if len(' '.join(current_words + [word])) <= max_tokens:\n",
    "                        current_words.append(word)\n",
    "                    else:\n",
    "                        chunks.append(' '.join(current_words))\n",
    "                        # 保留重叠部分（从尾部取单词）\n",
    "                        overlap_words = current_words[-overlap:] if overlap < len(current_words) else current_words\n",
    "                        current_words = overlap_words + [word]\n",
    "                if current_words:\n",
    "                    chunks.append(' '.join(current_words))\n",
    "            continue\n",
    "\n",
    "        # 正常句子处理\n",
    "        if len(current_chunk) + len(sentence) > max_tokens:\n",
    "            if chunks:\n",
    "                # 计算实际重叠长度（不超过剩余空间）\n",
    "                effective_overlap = min(\n",
    "                    overlap,\n",
    "                    len(current_chunk),\n",
    "                    max_tokens - len(sentence)\n",
    "                )\n",
    "                overlap_part = current_chunk[-effective_overlap:]\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = overlap_part + sentence\n",
    "            else:\n",
    "                # 第一个chunk直接超限\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # 后处理：确保所有chunk都不超限\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > max_tokens:\n",
    "            if lang == 'zh':\n",
    "                final_chunks.extend([chunk[i:i + max_tokens]\n",
    "                                     for i in range(0, len(chunk), max_tokens - overlap)])\n",
    "            else:\n",
    "                words = chunk.split()\n",
    "                current_words = []\n",
    "                for word in words:\n",
    "                    if len(' '.join(current_words + [word])) <= max_tokens:\n",
    "                        current_words.append(word)\n",
    "                    else:\n",
    "                        final_chunks.append(' '.join(current_words))\n",
    "                        current_words = [word]\n",
    "                if current_words:\n",
    "                    final_chunks.append(' '.join(current_words))\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_md(file_path: str) -> str:\n",
    "    \"\"\"提取MD文件中的文本内容\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        md_content = f.read()\n",
    "    \n",
    "    html = markdown2.markdown(md_content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_chunks_to_json(chunks: List[str], output_path: str) -> None:\n",
    "    \"\"\"将文本块保存为JSON文件\"\"\"\n",
    "    data = [{\"id\": i, \"text\": chunk, \"length\": len(chunk)} for i, chunk in enumerate(chunks)]\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_md_file(input_path: str, output_path: str, max_tokens: int = 400) -> None:\n",
    "    \"\"\"处理单个MD文件，分块并保存为JSON\"\"\"\n",
    "    try:\n",
    "        text = extract_text_from_md(input_path)\n",
    "        chunks = split_text_into_chunks(text, max_tokens=max_tokens)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        save_chunks_to_json(chunks, output_path)\n",
    "        \n",
    "        print(f\"已处理 {input_path}，生成 {len(chunks)} 个文本块\")\n",
    "        print(f\"保存到 {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {input_path} 时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_process_md_files(input_dir: str, output_dir: str, max_tokens: int = 400) -> None:\n",
    "    \"\"\"批量处理目录下的所有MD文件\"\"\"\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.md'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(input_path, input_dir)\n",
    "                output_filename = os.path.splitext(relative_path)[0] + '_chunks.json'\n",
    "                output_path = os.path.join(output_dir, output_filename)\n",
    "                \n",
    "                process_md_file(input_path, output_path, max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 ./datas/PSDF A Multi-Level Feature-Based Ponzi Scheme Detection Framework for Smart Contracts in Ethereum.md，生成 254 个文本块\n",
      "保存到 ./datas/output/PSDF A Multi-Level Feature-Based Ponzi Scheme Detection Framework for Smart Contracts in Ethereum_chunks.json\n",
      "已处理 ./datas/中国文化通史明国卷.md，生成 1008 个文本块\n",
      "保存到 ./datas/output/中国文化通史明国卷_chunks.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_DIR = \"./datas/\"  # MD文件目录\n",
    "    OUTPUT_DIR = \"./datas/output\"  # 输出JSON目录\n",
    "    \n",
    "    batch_process_md_files(INPUT_DIR, OUTPUT_DIR,400)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
