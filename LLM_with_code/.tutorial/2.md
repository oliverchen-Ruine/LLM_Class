将要进行的第一个实验是利用大规模语言模型理解和生成代码。自ChatGPT推出以来，这个简单的用例是实现人工智能代码助手的基础，其中GitHub Copilot是典型产品。


> **定义**:
GitHub Copilot是一款由人工智能驱动的工具，可帮助开发人员更高效地编写代码。它能分析代码和注释，并为个别行和整个函数提供建议。该工具由GitHub、OpenAI和微软共同开发，支持多种编程语言，可以执行代码补全、修改、解释和技术支持等各种任务。

在本实验中，将尝试使用三种不同的模型：FalconLLM, 由Technology Innovation Institute （TII）构建的用于摘要、文本生成和聊天机器人的大型语言模型；CodeLlama，MetaAI的Llama的微调版；StarCoder2，接下来的章节将研究的代码专用模型。

## 2.1 运行方式(ollama)

ollama是一个开源的、基于REST的、大规模语言模型服务器，通过下载ollama应用，可实现在本地机器上或在云中运行，可下载模型依赖于GPU的性能。

<p align="center">
<img src="/img/2.1.png" >
</p>

例如：在ollama中启动codellama:7b模型的运行方式如下：

```bash
ollama pull codellama:7b
ollama run codellama:7b
```

<p align="center">
<img src="/img/2.2.png" >
</p>

## 2.2 运行方式(HuggingFaceHub)
如果在本地机器上无法支持这些模型运行起来，为此我将使用带有GPU虚拟机的HuggingFaceHub推理端点。可以为每个推理端点链接一个模型，然后将其嵌入代码中，或者使用LangChain中的便捷库HuggingFaceEndpoint。

可以使用以下代码启用推理端点:
```python
llm = HuggingFaceEndpoint(endpoint_url = "yourEndpoint_url", task = 'text- generation', model_kwargs = {"max_tokens": 1100})
```
或者，也可以复制并粘贴端点网页上提供的Python代码，网址为https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name 。HuggingFace推理站点的用户调用如下。

```python
import requests

api_url = "https://api-inference.huggingface.co/models/uer/gpt2-chinese-cluecorpussmall"
api_token = "your_api_token" ##*号处省略7个字母
headers = {"Authorization":f"Bearer {api_token}"}

response = requests.post(api_url,headers=headers,json={"inputs":"你好，hf"})
print(response.json())

```

要创建HuggingFace推理端点，可以按照说明进行操作，网址为https://huggingface.co/docs/inference-endpoints/index。我们可以随时利用免费的HuggingFace API，但在运行模型时，必须预计到存在一定的延迟。