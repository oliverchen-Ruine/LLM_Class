## 目的：
- 文本清洗与分片（chunking）
- 把从 txt / pdf / docx 提取出来的长文本：
- 清洗干净（去空行、多余空格）
- 切分成小段（每段几百字），为后续 Embedding 做准备

## 为什么要分段？
>分段可以更有效地帮助大模型召回与用户查询最相关的内容，从而提升回复的准确性。合理的分段对回复的效果有着直接影响。如果分段太大，可能包含太多不相关的信息，从而降低了检索的准确性。相反，分段太小可能会丢失必要的上下文信息，导致生成的响应缺乏连贯性或深度。
* 提高检索效率：通过分段，将长篇文档拆解为多个短小的段落，使得检索系统能够快速定位到与用户查询最相关的段落，提高检索的速度和效率。
* 增强匹配精准度：分段后的文本单元更贴近用户查询的语义范围，有助于检索系统更准确地匹配关键词和语义，提升检索结果的相关性和精准度。
* 优化知识表示：分段有助于将复杂的知识内容拆解成更易于理解和处理的小单元，使得知识库中的知识结构更加清晰，便于后续的知识管理和应用。
* 改善用户体验：为用户提供更加精准和针对性的答案，提升用户与知识库交互的体验，满足用户快速获取所需信息的需求。

### 默认切分方式：

* 每段控制在 300～500 汉字左右（或 200～500 tokens）
* 可以加上重叠度保证段落之间有点上下文重合
>分段重叠度可以在一定程度上保留段落之间的上下文信息，避免因分段导致信息的割裂。这对于需要考虑上下文关系的文本处理任务（例如语义理解、上下文推理）尤为重要。

## 文档预处理标准化流程
1. **格式检测**：自动识别文档格式类型
2. **路由处理**：调用对应格式处理器
3. **统一清洗**：应用通用文本清洗规则
4. **语义分段**：按主题/长度智能分段
5. **元数据标注**：添加来源/类型等元信息
6. **存储优化**：转换为向量数据库友好格式

## 实操代码（适用于 txt/pdf/docx 提取出的长文本）：

### **分片函数**
```python
import re
import os
import json
import markdown
from bs4 import BeautifulSoup
from typing import List, Dict


def detect_language(text: str) -> str:
    """自动检测文本主要语言（中文/英文）"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    return 'zh' if chinese_chars > english_chars else 'en'

def split_text_into_chunks(
        text: str,
        max_tokens: int = 500,
        overlap: int = 50,
        language: str = None
) -> List[str]:
    """将长文本切分为多个chunk，保留单词完整性并支持重叠

    Args:
        text: 输入文本
        max_tokens: 每个chunk的最大长度
        overlap: 相邻chunk之间的重叠长度
        language: 可选强制指定语言('zh'/'en')，默认自动检测

    Returns:
        切分后的chunk列表
    """
    # 清理文本并检测语言
    cleaned = re.sub(r'\s+', ' ', text).strip()
    if not cleaned:
        return []

    lang = language or detect_language(cleaned)

    # 中英文不同的分割逻辑
    if lang == 'zh':
        # 中文按句子分割（保留标点）
        sentences = [s for s in re.split(r'(?<=[。！？.!?])', cleaned) if s]
    else:
        # 英文按句子分割（保留单词完整性）
        sentences = [s for s in re.split(r'(?<=[.!?])\s+', cleaned) if s]

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        # 处理超长句子（超过max_tokens）
        if len(sentence) > max_tokens:
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = ""

            if lang == 'zh':
                # 中文按字符分割（保留重叠）
                chunks.extend([sentence[i:i + max_tokens]
                               for i in range(0, len(sentence), max_tokens - overlap)])
            else:
                # 英文按单词分割（保留单词完整性）
                words = sentence.split()
                current_words = []
                for word in words:
                    if len(' '.join(current_words + [word])) <= max_tokens:
                        current_words.append(word)
                    else:
                        chunks.append(' '.join(current_words))
                        # 保留重叠部分（从尾部取单词）
                        overlap_words = current_words[-overlap:] if overlap < len(current_words) else current_words
                        current_words = overlap_words + [word]
                if current_words:
                    chunks.append(' '.join(current_words))
            continue

        # 正常句子处理
        if len(current_chunk) + len(sentence) > max_tokens:
            if chunks:
                # 计算实际重叠长度（不超过剩余空间）
                effective_overlap = min(
                    overlap,
                    len(current_chunk),
                    max_tokens - len(sentence)
                )
                overlap_part = current_chunk[-effective_overlap:]
                chunks.append(current_chunk)
                current_chunk = overlap_part + sentence
            else:
                # 第一个chunk直接超限
                chunks.append(current_chunk)
                current_chunk = sentence
        else:
            current_chunk += sentence

    if current_chunk:
        chunks.append(current_chunk)

    # 后处理：确保所有chunk都不超限
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > max_tokens:
            if lang == 'zh':
                final_chunks.extend([chunk[i:i + max_tokens]
                                     for i in range(0, len(chunk), max_tokens - overlap)])
            else:
                words = chunk.split()
                current_words = []
                for word in words:
                    if len(' '.join(current_words + [word])) <= max_tokens:
                        current_words.append(word)
                    else:
                        final_chunks.append(' '.join(current_words))
                        current_words = [word]
                if current_words:
                    final_chunks.append(' '.join(current_words))
        else:
            final_chunks.append(chunk)

    return final_chunks

```
### 从各种类型的文件中提取出纯文本

#### 从 PDF 文件中提取出纯文本函数
```python
def extract_text_from_pdf(file_path: str) -> str:
    """提取 PDF 中所有文本内容"""
    doc = fitz.open(file_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text()
    return full_text
```
#### 提取TXT文件中的文本内容函数
```python
def extract_text_from_txt(file_path: str) -> str:
    """提取TXT文件中的文本内容"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()
```
#### 提取CSV文件中的文本内容函数
```python
def extract_text_from_csv(file_path: str, delimiter: str = ',', text_columns: List[int] = None) -> str:
    """
    提取CSV文件中的文本内容
    
    参数:
        file_path: CSV文件路径
        delimiter: 分隔符，默认为逗号
        text_columns: 需要提取的文本列索引列表，默认为None(提取所有列)
    """
    full_text = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter=delimiter)
        for row in reader:
            if text_columns:
                row_text = [row[i] for i in text_columns if i < len(row)]
                full_text.append(' '.join(row_text))
            else:
                full_text.append(' '.join(row))
    return '\n'.join(full_text)
```
#### 提取Word(.docx)文件中的文本内容函数
```python
def extract_text_from_word(file_path: str) -> str:
    """提取Word(.docx)文件中的文本内容"""
    doc = Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)
```
#### 提取MD文件中的文本内容函数
```python
def extract_text_from_md(file_path: str) -> str:
    """提取MD文件中的文本内容"""
    with open(file_path, 'r', encoding='utf-8') as f:
        md_content = f.read()
    
    html = markdown.markdown(md_content)
    soup = BeautifulSoup(html, 'html.parser')
    return soup.get_text()
```

### **保存为带元数据的JSON格式函数**
```python

def save_chunks_to_json(chunks: List[str], output_path: str) -> None:
    """将文本块保存为JSON文件"""
    data = [{"id": i, "text": chunk, "length": len(chunk)} for i, chunk in enumerate(chunks)]
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

```

### 处理单个MD文件，分块并保存为JSON函数
```python
def process_md_file(input_path: str, output_path: str, max_tokens: int = 400) -> None:
    """处理单个MD文件，分块并保存为JSON"""
    try:
        text = extract_text_from_md(input_path)
        chunks = split_text_into_chunks(text, max_tokens=max_tokens)
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        save_chunks_to_json(chunks, output_path)
        
        print(f"已处理 {input_path}，生成 {len(chunks)} 个文本块")
        print(f"保存到 {output_path}")
    except Exception as e:
        print(f"处理文件 {input_path} 时出错: {e}")
```

### 批量处理目录下的所有MD文件函数
```python
def batch_process_md_files(input_dir: str, output_dir: str, max_tokens: int = 400) -> None:
    """批量处理目录下的所有MD文件"""
    for root, _, files in os.walk(input_dir):
        for file in files:
            if file.lower().endswith('.md'):
                input_path = os.path.join(root, file)
                relative_path = os.path.relpath(input_path, input_dir)
                output_filename = os.path.splitext(relative_path)[0] + '_chunks.json'
                output_path = os.path.join(output_dir, output_filename)
                
                process_md_file(input_path, output_path, max_tokens)
```

### **示例用法：**

```python

if __name__ == "__main__":
    INPUT_DIR = "../Datas/output_files_v4"  # MD文件目录
    OUTPUT_DIR = "../Output"  # 输出JSON目录
    
    batch_process_md_files(INPUT_DIR, OUTPUT_DIR)
```

## 实践建议：
* 一段 chunk 控制在 300～500 汉字最合适
* 太长会导致向量失真，太短会让语义不完整，确保每个段落包含的信息量适中，便于理解和分析
* 对英文可考虑基于 tokenizer 切词

## 输出结构建议：

可以将切分后的内容保存为一个 JSON 文件，例如文件[PSDF_thereum_chunks.json](../code/PSDF_thereum_chunks.json)：
```

[
  {
    "id": 0,
    "text": "绪言 1911年，以孙中山为首的资产阶级革命派领导了伟人的辛亥革命，推翻了清王朝，结束了2000余年的封建帝制，将中华民族引入到一个新的历史时代。1912年1月1日，中华民国临时政府成立，同年2月12口，隆裕太后发布诏书，正式宣布清帝逊位。史学界习惯上将辛亥革命的爆发之年视为民国的开端。从这年开始，到1949年新中国诞生为止，史称中华民国时期。本卷所要叙述的就是这一阶段中国文化发展的历史。 民国建立后，由于资产阶级力量先天不足，革命果实很快被袁世凯所篡夺。中国从此陷入了北洋军阀的黑暗统治之中，长达16年之久。袁世凯死后，直、皖、奉三系军阀交替控制政权，成为各帝国主义势力的在华代理人。大大小小的封建军阀不仅彼此之间争权夺利，混战不断，而H对尊孔复占情有独钟，弄得中国社会乌烟气，中华民国有名无实。",
    "length": 351
  },
  {
    "id": 1,
    "text": " 为反抗帝国主义的侵略和北洋军阀的黑暗统治，中国的先进分子掀起了波澜壮阔的新文化运动和气壮山河的五四爱国运动，将戊戌、辛亥时期所开启的思想文化启蒙和政治革命全面推向深入。五四运动后，马克思主义在中国得到广泛传播，并和新兴的中国工人运动相结合，产生了中国共产党。1924年，国共两党第--次合作，领导了反帝反封建的国民革命，北洋军阀的反动统治土崩瓦解。但是，以蒋介石为首的国民党右派于1927年背叛革命，残杀共产党人和革命群众，致使国民革命最终失败。1928年，国民党征服或笼络了各派军阀，确立了其在全国的统治地位，实现了中国形式上的-统局面。 面对国民党的专制独裁和残酷屠杀，中国共产党人被追进行了武装反抗。他们在各地发动起义，创立人民军队，建立了农村革命根据地，为实现白己的理想进行艰苦卓绝的斗争。",
    "length": 350
  },
  {
    "id": 2,
    "text": "装反抗。他们在各地发动起义，创立人民军队，建立了农村革命根据地，为实现白己的理想进行艰苦卓绝的斗争。就在国民党对这些根据地大肆进行围剿的时候，日本帝国主义乘虚而入，先是侵占中国东北，建立伪满洲国，进而发动全面侵华战争。民族矛盾因之空前激化。大敌当前，中国共产党不计前嫌，呼吁停止内战，一致抗日，得到了全国人民广泛而强烈的响应。国民党迫于民族人义，与共产党实行了第二次合作。这样，1937年卢沟桥事变发生后，中国就开始了全民族的抗日战争。经过8年的浴血奋战，终于打败了日本侵略者，获得了中华民族的新生，并为世界反法西斯战争的胜利作出了卓越贡献。 抗战胜利后，蒋介石国民党和平统一、民主建国的全民族愿望于不顾，从1946年起悍然发动了对解放区的全面进攻。解放区军民则针锋相对，给以有力的反击，仅仅用了三年时间，人民解放军就打垮了国民党的800万军队，致使其残余势力狼狈逃往台湾。",
    "length": 387
  },
  ......
]
```


> **文档处理是知识库构建的基石**，如同炼金术中的提纯过程，决定了最终知识的纯度与价值。