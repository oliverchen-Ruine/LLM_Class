**检索引擎负责从海量知识库中找到与用户问题最相关的文档片段**。但单个文档往往信息有限、碎片化，为了帮助生成模型理解，必须把多个相关上下文拼接起来，构造成一个“知识上下文”，一并传给生成模型。

* **为什么拼接？**
  生成模型理解的上下文有限（一般数千token），拼接多条检索结果补充信息，使答案更完整、更准确。
* **拼接方式**
  常用做法是将top\_k条最相关文档块，以换行分隔拼接，并在前后加提示语，让模型“知道这是背景知识”。
* **风险**
  拼接过多超出最大上下文长度，会导致截断，丢失信息。需要合理设置top\_k和文本长度限制。

---
### 代码

核心代码片段（RAGSystem类的 `query` 方法）：

```python
class RAGSystem:
    def __init__(self, index_path, temperature=0.8, model_name="qwen3:8b"):
        self.index, self.metadata = load_data_group(index_path)
        self.chat_history = deque(maxlen=6)  # 保留3轮对话
        self.temperature = temperature  # 存储温度值
        self.model_name = model_name  # 生成模型名称
    
    def _retrieve(self, query: str, top_k=5):
        """向量检索"""
        emb = get_embedding(query)
        distances, indices = self.index.search(np.array([emb]), top_k)
        return [self.metadata[idx] for idx in indices[0]]
    
    def _build_prompt(self, context_docs, query):
        """知识增强的Prompt构造"""
        context = "\n\n".join(f"【知识片段 {i}】{doc['text']}" 
                             for i, doc in enumerate(context_docs))
        return f"背景知识：\n{context}\n\n问题：{query}\n回答要求：用中文回答，包含知识来源"
    
    def generate_answer(self, query, top_k):
        # 1. 检索上下文
        context_docs = self._retrieve(query,top_k)


        # 2. 构造Prompt（包含多轮历史）
        prompt = "你是有专业文档支持的助手。请根据以下知识和对话历史回答问题：\n\n"
    
        # 添加对话历史（如果有）
        if self.chat_history:
            for i, content in enumerate(self.chat_history):
                role = "用户" if i % 2 == 0 else "助手"
                prompt += f"{role}: {content}\n"
    
        # 添加当前问题和知识片段
        prompt += f"\n知识背景：\n{self._build_prompt(context_docs, query)}"
        print("=== 最终 prompt ===")
        print(prompt)
        # 3. 调用Ollama
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,  # 明确要求非流式输出
                "options": {
                    "temperature": self.temperature  # 设置温度值
                }
            }
        )
        answer = response.json()["response"]
        
        # 4. 后处理
        clean_answer = self._postprocess(answer)
        self.chat_history.extend([query, clean_answer])
        return clean_answer


    def _postprocess(self, text):
        """答案优化"""
        last_query = self.chat_history[-2] if len(self.chat_history) >= 2 else ""
        # 学术文献添加引用
        if any(kw in last_query  for kw in ["文献", "研究"]):
            return f"{text}\n\n来源：{self.current_sources()}"
            
        # 法律文件精确条款标注
        elif "条款" in last_query :
            return re.sub(r"第([零一二三四五六七八九十百]+)条", r"【\g<0>】", text)
        
        return text.strip()

```

* 代码从`chat_history`中取文本字段，拼成一长串`context_docs`
* 用换行分隔每段，保证文本块之间明显分隔
* 加入提示语“背景知识：......回答要求：用中文回答，包含知识来源”明确告诉模型的参考资料与输出方式
* 拼接完上下文后，再附加用户问题，形成完整Prompt给生成模型

---

### 为什么要这么做

* **模型需要上下文**：生成模型是基于上下文概率生成回答，清晰丰富的上下文能大幅提升回答质量。
* **多文档拼接避免信息缺失**：单条检索结果可能遗漏关键信息，多条拼接保证更全面。
* **明确指示模型任务**：提示语让模型聚焦“根据资料回答”，避免胡乱发挥。
* **模型注意力机制限制**：生成模型（如 qwen-plus）的注意力机制对长文本的处理存在 “遗忘效应”，靠前的文本块对回答的影响可能更显著，因此拼接时可按相似度倒序排列（最相关的放前面）。
* **减少幻觉风险**：明确的提示语（如 “根据以下资料回答”）能强制模型优先使用检索到的上下文，而非内部知识库，降低生成错误信息的概率（幻觉）。

---

### 解决什么问题

* **回答准确性和丰富度提升**：上下文信息多，模型可用知识面广，生成更准确详细答案。
* **避免断章取义**：拼接多个片段减少单条文档可能带来的片面解释。
* **方便系统拓展**：检索模块和生成模块清晰解耦，方便后期调整和优化。

---

### 可调节点

* **top_k数目**
  直接影响上下文长度和信息量，增大top\_k提升答案丰富度，但也可能引起响应延迟和上下文超长截断。建议根据模型最大上下文长度调节，一般5-10为宜。
* **文本拼接格式**
  可以用序号、分隔符、标签（如“资料1：”）标明不同文本来源，帮助模型理清结构。
* **上下文长度截断策略**
  针对超长上下文，可先计算总token数，超过阈值后从后往前截断或优先保留距离最近的文本块。
* **提示语设计**
  设计不同的系统提示，可以引导模型回答风格，如更专业、更口语化或更简洁。

---