## 1.Embedding核心原理
**Embedding 是一种将文本、图像、音频等离散数据映射为连续向量空间中的方法。**
### 1.1 例如：
```text
输入： "自然语言处理技术"
输出： [0.23, -0.15, 0.78, -0.34, ..., 0.09] （1024维）
```
为了实现智能问答、语义搜索、相似内容推荐等功能，需要一种方法把文本转换成机器能够理解和比较的形式。Embedding（嵌入）技术就是实现这一目标的核心工具。

由模型根据其语义和上下文信息生成的，具有“语义上的距离感知能力”：
* 向量之间的距离越近 → 表示语义越相似 
* 向量之间的距离越远 → 表示语义差异较大

>描述“Python 编程”的文本和描述“编程语言”的文本**在空间中的差异要小于**描述“美食”的文本与“机器学习”的文本
### 1.2 常用的 Embedding 模型

| 模型名称                          | 维度         | 说明                   |
| ----------------------------- | ---------- | -------------------- |
| OpenAI text-embedding-3-small | 1536       | 高质量通用语义向量            |
| BGE（BAAI General Embedding）   | 768 / 1024 | 中文表现优秀               |
| MiniLM / Instructor           | 可选         | 轻量级嵌入模型，适合本地部署       |
| GTE                           | 384 / 768  | HuggingFace 支持，本地效果好 |
| E5、S-BERT                     | 384+       | 多语言支持                |
| Qwen3                        | 1024       | 高质量中文向量              |


## 2.Qwen3 Embedding 模型概览
Qwen3 Embedding 系列基于 Qwen3 基础模型构建，采用了先进的双编码器和交叉编码器架构。通过 LoRA 微调技术，充分保留并增强了基础模型的文本理解能力。

|模型类型	|模型名称	|参数量	|层数	|序列长度	|嵌入维度	|MRL 支持	|指令感知|
|----------|----------|----------|----------|----------|----------|----------|----------|
|文本嵌入	|Qwen3-Embedding-0.6B	|0.6B	|28	|32K	|1024	|✅	|✅|
|文本嵌入	|Qwen3-Embedding-4B	4B	|36	|32K	|2560	|✅	|✅|
|文本嵌入	|Qwen3-Embedding-8B	8B	|36	|32K	|4096	|✅	|✅|
|文本重排	|Qwen3-Reranker-0.6B	|0.6B	|28	|32K	|-	|-	|✅|
|文本重排	|Qwen3-Reranker-4B	|4B	|36	|32K	|-	|-	|✅|
|文本重排	|Qwen3-Reranker-8B	|8B	|36	|32K	|-	|-	|✅|

>MRL Support表示嵌入模型是否支持最终嵌入的自定义维度。指令感知表示嵌入或重新排序模型是否支持根据不同的任务定制输入指令

### 2.1 准备工作
启动ollama服务，并

```python
ollama start
```
拆分终端

<img src="/img/2.2.1.png" >

在开发环境配置中检查是否有Qwen3-Embedding-8B模型

<img src="/img/2.2.2.png" >


如果提示:

```python
No model instance found for model: qwen3-embedding-8b
```

请在控制台拉去并创建模型实例:

```python
ollama pull dengcao/Qwen3-Embedding-8B:Q5_K_M
```

### 2.2 调用Qwen3 Embedding模型
以下是使用Python代码调用Qwen3 Embedding模型生成文本向量的示例：
```python
from ollama import Client
import numpy as np
 
client = Client(host='http://localhost:11434')
 
def get_embedding(text, model="dengcao/Qwen3-Embedding-8B:Q5_K_M"):
    response = client.embeddings(model=model, prompt=text)
    return np.array(response['embedding'], dtype=np.float32)
 
# 批量处理
texts = ["这是一个embedding的示例"]
embeddings = [get_embedding(text) for text in texts]
print(embeddings)

```
- 输出文本向量示例：

```python
[array([ 0.39460367,  0.9794821 ,  2.2050304 , ..., -0.1629711 ,
       -0.08993335,  1.0394384 ], dtype=float32)]
```

### 2.3 关键参数说明
| 参数 | 必填 | 说明 |
|------|------|------|
| `model` | 是 | 模型名称（需提前`ollama pull dengcao/Qwen3-Embedding-8B:Q5_K_M`） |
| `prompt` | 是 | 输入文本（建议不超过512token） |
| `encoding_format` | 否 | 默认float，可选base64 |
| `dimensions` | 否 | 输出维度（不指定则用模型默认） |
| `keep_alive` | 否 | 控制模型加载时长（如"5m"） |