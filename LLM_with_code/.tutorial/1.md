# 1.1 选择正确大规模语言模型的决策框架
目前市场上存在一些最有前途的大规模语言模型。但现在的问题是: 应该在应用中使用哪一种模型? 事实上, 这个问题并没有直接的答案。

## 考虑因素

在为应用选择大规模语言模型时, 需要考虑很多因素。这些因素还需要在两种场景下进行权衡: 专有大规模语言模型和开源大规模语言模型。以下是我么在选择大规模语言模型时可能需要考虑的一些因素和权衡。

- **规模和性能:** 更复杂的模型(即参数数量较多)往往具有更好的性能, 尤其是在参数知识和泛化能力方面。然而, 模型越大, 处理输入和生成输出所需的计算量和内存就越大, 这可能会导致产生更高的延迟, 成本也会更高。

- **成本和托管策略:** 将大规模语言模型纳入应用程序时, 必须牢记以下两类成本。

    - **模型消耗成本:** 指的是为使用模型而支付的费用。像 GPT-4 或 Claude2 这样的专有模型需要付费, 费用通常与处理的词元数量成正比。而 LlaMA 或 FalconLLM 等开源模型则可以免费使用。

    - **模型托管费用:** 指的是托管策略。通常情况下, 专有模型托管在私有或公共超级分级器中, 这样就可以通过 REST API 使用这些模型, 而不必担心底层基础设施(例如, GPT-4 托管在微软 Azure 云构建的超级计算机中)。对于开源模型, 通常需要提供自己的基础设施, 因为这些模型可以下载到本地。当然, 模型越大, 所需的计算力就越强。

> **注意：**
就开源模型而言, 使用这些模型的方式一个是ollama, 它是一个开源的、基于 REST 的大规模语言模型服务器, 可以在本地运行, 并且可以托管在云中。ollama 的优点是, 它可以让你在本地运行模型，通过在其官网可搜索开源模型，并且可以托管在云中。缺点是, 它需要你自行托管基础设施, 并且需要你自行处理计算和存储成本。如果你需要使用一个开源模型, 并且需要托管自己的基础设施, 那么 ollama 可能是一个不错的选择。另一个选择是使用 Hugging Face Inference API。免费版本允许在 Hugging Face 托管的共享基础设施上以有限的速率测试和评估所有可用的大规模语言模型。对于生产用例, Hugging Face 还提供推理端点, 支持轻松地将大规模语言模型部署到专用和全面管理的基础设施上, 并可配置区域、计算力和安全级别等参数, 以适应延迟、吞吐量和合规性方面的限制。推理端点的公开定价为https://huggingface.co/docs/inference-endpoints/pricing。

- **定制**: 这可能是我们在决定采用哪种模式之前需要评估的一项要求。事实上, 并非所有模型在定制方面都同样灵活。当谈到定制时, 指的是以下两类活动。

    - **微调**: 指的是稍微调整大规模语言模型参数以更好地适应某个领域的过程。所有开源模型都可以进行微调。就专有模型而言, 并非所有大规模语言模型都可以进行微调: 例如, OpenAI 的 GPT-3.5 可以进行微调, 而 GPT-4-0613 的微调过程仍处于实验阶段, 需要向 OpenAI 申请(截至 2023 年 12 月)。

    - **从头开始训练**: 如果真的想要得到一个关于自己领域知识的超级具体的大规模语言模型, 就可能需要从头开始重新训练模型。要从头开始训练大规模语言模型, 不必重新设计架构, 只需下载开源大规模语言模型, 然后在自定义数据集上对其进行重新训练即可。当然, 这意味着可以访问源代码, 但是在使用专有大规模语言模型时, 情况并非如此。

- **特定领域的能力**: 评估大规模语言模型性能的最常用方法是对不同领域的不同基准取平均值。不过, 也有一些基准是针对特定能力量身定制的: 如 MMLU 可衡量大规模语言模型的广义文化和常识推理能力, TruthfulQA 则更关注大规模语言模型的对齐能力, 而 HumanEval 则针对大规模语言模型的编码能力。

因此, 如果有一个量身定制的用例, 则我们可能希望使用一个在特定基准中表现最佳, 而非在所有基准中平均表现最佳的模型。例如, 想要获得卓越的编码能力, 可以选择 Claude 2; 更看重分析推理能力, 可以选择 PaLM 2。另一方面, 若需要一个能涵盖所有这些功能的模型, GPT-4 则可能是理想选择。

选择特定领域的模型也是节省模型复杂度的一种方法。问题是, 如果需要将一个相对较小的模型(例如 LlaMA- 7B- instruct)用于特定的用例, 那么使用该模型可能就足够了, 毕竟该模型在成本和性能方面都具有优势。

> **注意：**
如果我们正在寻找极其特殊的大规模语言模型, 那么有大量模型都是根据特定领域的技术文档训练出来的。例如, 在 2023 年初, 斯坦福大学基础模型研究中心(Center for Research on Foundation Models, CRFM)和 MosaicML 宣布发布 BioMedLM, 这是一个基于仅解码器 transformer 的大规模语言模型, 拥有 27 亿个参数, 以生物医学摘要和论文为训练对象。另一个例子是 BloombergGPT, 它是由彭博社开发的专门针对金融领域的 500 亿参数大规模语言模型, 基于彭博社广泛的数据源, 在 3,630 亿词元数据集上对其进行训练, 这可能是迄今为止最大的特定领域数据集, 并从通用数据集中扩充了 3,450 亿词元。

为了让这个决策框架更加实用, 接着来看一看下面这个关于 TechGen 公司的假想案例研究。

## 一个案例

TechGenSolutions是一家领先的人工智能驱动分析提供商, 他们的下一代客户交互系统面临着在两种高级语言模型之间进行抉择: GPT- 4 和 LLaMa- 2。他们需要使用一个鲁棒的语言模型, 用于处理各种客户查询, 提供准确的技术信息, 并与其专有的软件集成。以下是他们的选择。

- **GPT-4**: 由 OpenAI 开发, 以其庞大的参数数量和处理文本与图像输入的能力而著称。
- **LLama 2**: LLama 2 由 Meta AI 开发, 是一个开源模型, 因其在较小数据集上具有易用性和高性能而备受赞誉。

以下是他们做出决定时考虑的因素。

- **性能**: TechGen 对模型的性能进行了评估, 尤其是在生成技术内容和代码方面, GPT-4 表现出了更高的准确性。

- **集成性**: 与 TechGen 系统集成的难易程度至关重要, GPT-4 因其广泛采用而可能提供更无缝的兼容性。

- **成本**: LLama 2 在某些条件下可免费用于商业用途, 而 GPT-4 则需要花费成本, TechGen 必须在决策中考虑到这一点。

- **面向未来**: TechGen 会考虑每种模式的长期可行性, 包括更新和改进的可能性。

基于这些考虑, TechGen 选择了 GPT- 4, 因为它在生成复杂的技术性响应方面表现出色, 而且兼具多语言功能, 符合 TechGen 的国际扩张计划。这一决定还受到 GPT- 4 的图像处理功能的影响, TechGen 预计, 随着他们在客户服务中加入越来越多的多媒体内容, 这一功能将变得越来越重要。TechGen之所以选择GPT- 4而非LLama2,是因为他们需要使用一个高性能、多用途的语言模型,以适应其不断增长的全球业务和多样化的客户需求。虽然LLama2的开源性和成本效益很有吸引力,但GPT- 4的先进功能和面向未来的特性对于TechGen的宏伟目标来说更具说服力。

### 注意

这些决策因素并不是决定在应用程序中嵌入哪些模型的详尽指南。不过,它们都是在建立应用流程时值得思考的有用因素,这样我们就可以确定自己的需求,然后筛选出更适合自己目标的大规模语言模型。

# 1.2 评估大规模语言模型编码能力的指标

一般来说，所有大规模语言模型都具备代码理解和生成方面的知识；但是，有些大规模语言模型在这方面特别专业。更具体地说，有一些评估基准(如HumanEval)专门用于评估大规模语言模型处理代码的能力。EvalPlus 排行榜是确定表现最佳的模型的一个很好的来源，网址为https://evalplus.github.io/leaderboard.html 。其中 <a href="https://github.com/openai/human-eval" target="_blank" >HumanEval</a> 是OpenAI推出的一项基准测试，用于评估大规模语言模型的代码生成能力。<a href="https://github.com/google-research/google-research/tree/master/mbpp" target="_blank">MostlyBasic Programming Problems(MBPP)</a>是另一个评估代码生成能力的基准测试。它包含974道编程问题，适合初级程序员解决。它们已被用于评估Codex等模型，证明了其在衡量功能正确性方面的有效性。
<p align="center">
<img src="/img/1.1.png" width="80%" height="80%">
</p>

如上所示，大多数模型都是GPT- 4的微调版本(以及GPT- 4本身)，因为它基本上是所有领域中技术最先进的大规模语言模型。不过，也有许多开源模型在代码理解和生成领域取得了令人惊叹的成果，其中一些模型将在接下来的章节中介绍。另一个基准是“最基本编程问题”(Mostly Basic Programming Problem, MBPP)，这是一个包含974个Python编程任务的数据集，旨在让入门级程序员都能着手解决。因此，在为特定代码任务选择模型时，不妨参考一下这些基准以及其他类似的代码指标(本章将进一步介绍一些针对特定代码的大规模语言模型基准)。

在编码领域内，市场上还有常用的另外三种基准。

- MultiPL-E: HumanEval 到许多其他语言(如Java、C#、Ruby和SQL)的扩展。
- DS-1000: 测试模型是否能用Python为常见数据分析任务编写代码的数据科学基准。
- 技术助理提示: 测试模型是否能充当技术助理并回答编程相关请求的提示。

我们将测试不同的大规模语言模型: 两个代码专用大规模语言模型(CodeLlama和StarCoder)和一个通用大规模语言模型(FalconLLM)，后者也具有代码生成领域的新兴功能。